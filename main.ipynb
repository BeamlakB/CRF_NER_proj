{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950f939-0462-4808-a13e-c7677cb06058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name: Beamlak Bekele\n",
    "Data: 1/25/2024\n",
    "Description: Trained a model to recognize names in informal text, such as emails using Enron corpus data,which contains over 700 emails.\n",
    "Used Conditional random fields as  a modeling method.\n",
    "\n",
    "Tried to replicate the result from:  http://www.cs.cmu.edu/~einat/email.pdf this paper \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76c0a2-5060-401b-bb6a-ead9da255f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import intialization \n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import configparser\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import configparser\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import email\n",
    "import mailparser\n",
    "import xml.etree.ElementTree as ET\n",
    "try:\n",
    "\tfrom talon.signature.bruteforce import extract_signature\n",
    "except:\n",
    "\t\n",
    "\tfrom talon.signature.bruteforce import extract_signature\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887a012-17fc-4548-a88b-a2d3438754c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the local location of the data  \n",
    "mail_dir = './data/enronRandom-XML/train'\n",
    "test_dir = './data/enronRandom-XML/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f0718-1857-4bff-8520-9a3755f0bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All enron emails were sent using the Multipurpose internet Mail extension format. \n",
    "#using correct libaries and methods to clean the emails and put it a panda data base \n",
    "def process_email(index):\n",
    "    \"\"\"\n",
    "    This function splits a raw email into constituent parts that can be used a features\n",
    "    \"\"\"\n",
    "    email_path = index[0]\n",
    "    #print (email_path)\n",
    "    employee = index[1]\n",
    "    folder = index[2]\n",
    "    mail = mailparser.parse_from_file(email_path)\n",
    "    full_body = email.message_from_string(mail.body)\n",
    "\n",
    "    #only retrive the body of the email\n",
    "    if full_body.is_multipart():\n",
    "        retrun \n",
    "    else:\n",
    "        mail_body=full_body.get_payload()\n",
    "\n",
    "    split_body= clean_body(mail_body)\n",
    "    headers = mail.headers\n",
    "    #reformat date to be panda readable\n",
    "    date_time = process_date(headers.get('Date'))\n",
    "    email_dict = { \n",
    "                \"employee\" : employee,\n",
    "                \"email_folder\": folder,\n",
    "                \"message_id\": headers.get('Message-ID'),\n",
    "                \"date\" : date_time,\n",
    "                \"from\" : headers.get('From'),\n",
    "                \"subject\": headers.get('Subject'),\n",
    "                \"body\" : split_body['body'],\n",
    "                \"chain\" : split_body['chain'],\n",
    "                \"signature\": split_body['signature'],\n",
    "                #\"wholeEmail\": headers.get('Subject')+\" \"+ split_body['body'],\n",
    "                \"full_email_path\" : email_path #for debug purposes. \n",
    "    }\n",
    "    #append data frame\n",
    "    return email_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642e84c-be03-4740-b978-1ef6dbc5121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body(mail_body):\n",
    "    '''\n",
    "    This extracts both the email signature, and the forwarding email chain if it exists. \n",
    "    '''\n",
    "    delimiters = [\"-----Original Message-----\",\"To:\",\"From\"]\n",
    "    \n",
    "    #Trying to split string by biggest delimiter. \n",
    "    old_len = sys.maxsize\n",
    "    \n",
    "    for delimiter in delimiters:\n",
    "        split_body = mail_body.split(delimiter,1)\n",
    "        new_len = len(split_body[0])\n",
    "        if new_len <= old_len:\n",
    "            old_len = new_len\n",
    "            final_split = split_body\n",
    "            \n",
    "    #Then pull chain message\n",
    "    if (len(final_split) == 1):\n",
    "        mail_chain = None\n",
    "    else:\n",
    "        mail_chain = final_split[1] \n",
    "    \n",
    "    #The following uses Talon to try to get a clean body, and seperate out the rest of the email. \n",
    "    clean_body, sig = extract_signature(final_split[0])\n",
    "    \n",
    "    return {'body': clean_body, 'chain' : mail_chain, 'signature': sig}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f547cce-32f4-4175-b545-4d577ca595ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the MIME date format to panda friendly type\n",
    "def process_date(date_time):\n",
    "    try:\n",
    "        date_time = email.utils.format_datetime(email.utils.parsedate_to_datetime(date_time))\n",
    "    except:\n",
    "        date_time = None\n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7023b9-8327-4202-bba1-1390ac044449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_email_paths(mail_dir):\n",
    "    '''\n",
    "    Given a mail directory, this will generate the file paths to each email in each inbox. \n",
    "    '''\n",
    "    mailboxes = listdir(mail_dir)\n",
    "    #mail_dir = './date/'\n",
    "    #print (mailboxes)\n",
    "    for folder in mailboxes:\n",
    "        path = mail_dir+ \"/\" + folder\n",
    "        emails = listdir(path)\n",
    "        for single_email in emails:\n",
    "            #print(single_email)\n",
    "            full_path = path + \"/\" + single_email\n",
    "            name = single_email.split(\"_\",1)\n",
    "            if isfile(full_path): #Skip directories.\n",
    "                yield (full_path, name[0], folder)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eea906-f699-4621-bab8-9b6f769ebc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from os import listdir\n",
    "try:\n",
    "    cpus = mp.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 2\n",
    "pool = mp.Pool(processes=cpus)\n",
    "print(\"CPUS: \" + str(cpus))\n",
    "\n",
    "#Catagorizes parts of the email and a data frame for training data \n",
    "\n",
    "indexes = generate_email_paths(mail_dir)\n",
    "#print(indexes)\n",
    "enron_email_df = pool.map(process_email,indexes)\n",
    "#print(enron_email_df)\n",
    "#Remove Nones from the list\n",
    "enron_email_df = [i for i in enron_email_df if i]\n",
    "#print(enron_email_df)\n",
    "enron_email_df = pd.DataFrame(enron_email_df)\n",
    "\n",
    "########################################################\n",
    "\n",
    "#Catagorizes parts of the email and a data frame for testing data \n",
    "indexes = generate_email_paths(test_dir)\n",
    "#print(indexes)\n",
    "tenron_email_df = pool.map(process_email,indexes)\n",
    "#print(enron_email_df)\n",
    "#Remove Nones from the list\n",
    "tenron_email_df = [i for i in tenron_email_df if i]\n",
    "#print(enron_email_df)\n",
    "tenron_email_df = pd.DataFrame(tenron_email_df)\n",
    "#enron_email_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537e139-68c5-485c-a376-daa733583170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#729 rows Ã— 11 columns columns data in data frame \n",
    "#prints the data fram \n",
    "enron_email_df.describe()\n",
    "#tenron_email_df.describe()\n",
    "display (tenron_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db9aca-6de4-46f3-96f6-6269555d5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/true_name is one of the most common words because the corpus is labeled. The label is <true_name>. \n",
    "\"\"\"\n",
    "#To understand more about the email corpus we can \n",
    "#Find the most common words\n",
    "from collections import Counter\n",
    "bd = enron_email_df[\"body\"]         #the body column of the data frame \n",
    "print (Counter(\" \".join(bd).split()).most_common(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9def3-bea0-4f12-9599-43ee0a1e43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries will be used to prepare the data to train the model \n",
    "#this includes extracting feature, and identifying labels \n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Tag\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pycrfsuite\n",
    "import os, os.path, sys\n",
    "import glob\n",
    "from xml.etree import ElementTree\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c4b17-3d13-433b-a756-21b4cefe50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function \n",
    "#this function removes special characters and punctuations\n",
    "def remov_punct(withpunct):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    without_punct = \"\"\n",
    "    char = 'nan'\n",
    "    for char in withpunct:\n",
    "        if  str(char) not in punctuations:\n",
    "            without_punct = without_punct + str(char)\n",
    "    return(without_punct)\n",
    "\n",
    "# functions for extracting features in documents\n",
    "def extract_features(doc,t):\n",
    "    return [word2features(doc, i,t) for i in range(len(doc))]\n",
    "\n",
    "def get_labels(doc):\n",
    "    return [label for (token, postag, label) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0300c424-74f6-4625-adaf-1c2faf4a3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The files in the data directory are XML files. It needs to be changed beautiful soup\n",
    "#BS format will change email to HTML text style. T\n",
    "#Then it tags each word as name or not name \n",
    "\n",
    "#train data \n",
    "#array docs is 2d array: each row has tuples (\"word\", tag)\n",
    "#eg:[ ('Sincerely', 'NA'), ('Paula', 'true_name')]\n",
    "docs = []\n",
    "#array of all the word in the corpus \n",
    "sents = []\n",
    "\n",
    "#Just using 200 email from corpus \n",
    "for b in range (200):\n",
    "    sub= enron_email_df[\"body\"][b]\n",
    "    soup = bs(sub, \"html5lib\")\n",
    "    for d in soup.find_all(\"body\"):\n",
    "       for wrd in d.contents:    \n",
    "        tags = []\n",
    "        NoneType = type(None)   \n",
    "        if isinstance(wrd.name, NoneType) == True:\n",
    "            withoutpunct = remov_punct(wrd)\n",
    "            temp = word_tokenize(withoutpunct)\n",
    "            for token in temp:\n",
    "                tags.append((token,'NA'))            \n",
    "        else:\n",
    "            withoutpunct = remov_punct(wrd)\n",
    "            withoutpunct = remov_punct(withoutpunct) #remove twice to remove email attachment \n",
    "            temp = word_tokenize(withoutpunct)\n",
    "            for token in temp:\n",
    "                if (wrd.name == \"true_name\"):\n",
    "                    tags.append((token,wrd.name))  \n",
    "                else:\n",
    "                     tags.append((token,'NA'))   \n",
    "        sents = sents + tags \n",
    "       docs.append(sents)\n",
    " \n",
    "#test data \n",
    "#array docs is 2d array: each row has tuples (\"word\", tag)\n",
    "#eg:[ ('Sincerely', 'NA'), ('Paula', 'true_name')]\n",
    "tdocs = []\n",
    "#array of all the word in the corpus \n",
    "tsents = []\n",
    "\n",
    "#Prepare 160 email\n",
    "for b in range (160):\n",
    "    sub= tenron_email_df[\"body\"][b]\n",
    "    soup = bs(sub, \"html5lib\")\n",
    "    for d in soup.find_all(\"body\"):\n",
    "       for wrd in d.contents:    \n",
    "        tags = []\n",
    "        NoneType = type(None)   \n",
    "        if isinstance(wrd.name, NoneType) == True:\n",
    "            withoutpunct = remov_punct(wrd)\n",
    "            temp = word_tokenize(withoutpunct)\n",
    "            for token in temp:\n",
    "                tags.append((token,'NA'))            \n",
    "        else:\n",
    "            withoutpunct = remov_punct(wrd)\n",
    "            withoutpunct = remov_punct(withoutpunct) #remove twice to remove email attachment \n",
    "            temp = word_tokenize(withoutpunct)\n",
    "            for token in temp:\n",
    "                if (wrd.name == \"true_name\"):\n",
    "                    tags.append((token,wrd.name))  \n",
    "                else:\n",
    "                     tags.append((token,'NA'))   \n",
    "        tsents = tsents + tags \n",
    "       tdocs.append(tsents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322adec-05a6-42a9-af20-562974732ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell used to extract name from email address in the from column \n",
    "fromlist = []\n",
    "import xml.etree.ElementTree as ET\n",
    "sub= enron_email_df[\"from\"][6]\n",
    "print (sub)\n",
    "if (sub):\n",
    "    soup = bs(sub, \"html5lib\")\n",
    "    for d in soup.find_all(\"body\"):\n",
    "        for wrd in d: \n",
    "            wrd = wrd.text\n",
    "            if (wrd != None):\n",
    "                fromname = wrd.split(\"@\")[0].split(\".\")\n",
    "                for i in fromname:\n",
    "                    fromlist.append(i)\n",
    "   \n",
    "print (fromlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bdd16-3456-42c9-997e-d101aa767213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the subject, from, and signature part of the email in to a list of token\n",
    "#This will be used as a feature, since this section of emails usually are names or contain \n",
    "subjectlist=[]   #list of words in the subject\n",
    "fromlist=[]      #list of parsed names from the emails in from column of data frame\n",
    "signlist= []     #list of words from in the signature part of the email\n",
    "\n",
    "# training data data frame\n",
    "thedf= enron_email_df    \n",
    "for b in range (len (thedf[\"subject\"])):\n",
    "    sub= thedf[\"subject\"][b]\n",
    "    if (sub):\n",
    "        soup = bs(sub, \"html5lib\")\n",
    "        for d in soup.find_all(\"body\"):\n",
    "           for wrd in d.contents:    \n",
    "               withoutpunct = remov_punct(wrd)\n",
    "               temp = word_tokenize(without unit)\n",
    "               for token in temp:\n",
    "                   #add each word to subject list\n",
    "                    subjectlist.append(token)\n",
    "    sub= thedf[\"from\"][b]\n",
    "    if (sub):\n",
    "        soup = bs(sub, \"html5lib\")\n",
    "        for d in soup.find_all(\"body\"):\n",
    "            for wrd in d: \n",
    "                wrd= wrd.text\n",
    "                if (wrd != None):\n",
    "                    #remove @ from email address\n",
    "                    #split the username with \".\" as delimeter \n",
    "                    fromname = wrd.split(\"@\")[0].split(\".\")\n",
    "                    for i in fromname:\n",
    "                        #add split username to from list\n",
    "                        fromlist.append(i)\n",
    "    sub= thedf[\"signature\"][b]\n",
    "    if (sub):\n",
    "        soup = bs(sub, \"html5lib\")\n",
    "        for d in soup.find_all(\"body\"):\n",
    "            for wrd in d.contents: \n",
    "                withoutpunct = remov_punct(wrd)\n",
    "                temp = word_tokenize(withoutpunct)\n",
    "                for token in temp:\n",
    "                    #if the signature isn't number then add to signlist \n",
    "                    isnum=False\n",
    "                    for w in token:\n",
    "                        if w.isdigit():\n",
    "                            isnum=True\n",
    "                            break\n",
    "                    if not isnum:\n",
    "                        signlist.append(token)\n",
    "        \n",
    "print (signlist)     \n",
    "\n",
    "\n",
    "#Convert the subject, from, and signature part of the email in to a list for the test data \n",
    "tsubjectlist=[]\n",
    "tfromlist=[]\n",
    "tsignlist = {}\n",
    "thedf= tenron_email_df    \n",
    "for b in range (len (thedf[\"subject\"])):\n",
    "    sub= thedf[\"subject\"][b]\n",
    "    if (sub):\n",
    "        soup = bs(sub, \"html5lib\")\n",
    "        for d in soup.find_all(\"body\"):\n",
    "           for wrd in d.contents:    \n",
    "               withoutpunct = remov_punct(wrd)\n",
    "               temp = word_tokenize(withoutpunct)\n",
    "               for token in temp:\n",
    "                    tsubjectlist.append(token)\n",
    "    sub= thedf[\"from\"][b]\n",
    "    soup = bs(sub, \"html5lib\")\n",
    "    if (sub):\n",
    "        for d in soup.find_all(\"body\"):\n",
    "            for wrd in d: \n",
    "                wrd= wrd.text\n",
    "                fromname = wrd.split(\"@\")[0].split(\".\")\n",
    "                for i in fromname:\n",
    "                    tfromlist.append(i)\n",
    "    sub= thedf[\"signature\"][b]\n",
    "    if (sub):\n",
    "        soup = bs(sub, \"html5lib\")\n",
    "        for d in soup.find_all(\"body\"):\n",
    "            for wrd in d.contents: \n",
    "                withoutpunct = remov_punct(wrd)\n",
    "                temp = word_tokenize(withoutpunct)\n",
    "                for token in temp:\n",
    "                    tsignlist[token]= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d51fc-3989-452c-8920-e9be32016701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function extract features\n",
    "\n",
    "#if the token is a title \n",
    "def is_nametitle (word):\n",
    "    title_patterns = ['mr.', 'ms.', 'mrs.', 'dr.', 'prof.', '.jr', 'staff', 'rep', 'reporter','president','ceo','says']\n",
    "    word= word.lower()\n",
    "    return any(word.startswith(pattern) for pattern in title_patterns)\n",
    "\n",
    "# checks if word is in the common first name list in this directory \n",
    "def first_namedic (word):\n",
    "    with open(\"first-name.txt\" , 'r') as file:\n",
    "            # Read all lines from the file\n",
    "            lines = file.readlines()\n",
    "\n",
    "            # Check if the word is in any of the lines\n",
    "            for line in lines:\n",
    "                if word.lower() in line:\n",
    "                    return True\n",
    "\n",
    "        # If the word is not found in any line\n",
    "    return False\n",
    "\n",
    "\n",
    "# checks if word is in the common last name list in this directory \n",
    "def last_namedic (word):\n",
    "    with open(\"last-name.txt\" , 'r') as file:\n",
    "            # Read all lines from the file\n",
    "            lines = file.readlines()\n",
    "\n",
    "            # Check if the word is in any of the lines\n",
    "            for line in lines:\n",
    "                if word.lower() in line:\n",
    "                    return True\n",
    "\n",
    "        # If the word is not found in any line\n",
    "    return False\n",
    "\n",
    "#check if the word was in the header/subject of the email \n",
    "def is_inheader (word,t):\n",
    "    if (t == \"train\"):\n",
    "        mylist =subjectlist\n",
    "    else:\n",
    "         mylist =tsubjectlist\n",
    "    if (word in mylist):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#check if the word was in the list of names extract from the email address in from column of the data frame \n",
    "def is_infrom (word,t):\n",
    "    #mylist=fromlist\n",
    "    if (t == \"train\"):\n",
    "        mylist =fromlist\n",
    "    else:\n",
    "         mylist =tfromlist\n",
    "    if (word in mylist):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#check if the word was in the header/subject of the email \n",
    "def is_insign (word,t):\n",
    "    #mylist=signlist\n",
    "    if (t == \"train\"):\n",
    "        mylist =fromlist\n",
    "    else:\n",
    "         mylist =tfromlist\n",
    "    if (word in mylist):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e7103-5cf3-4a9d-bc1a-affe84a0f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function above and libraries generate feauters \n",
    "\n",
    "def word2features(doc, i,t):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "\n",
    "    # Common features for all words. You may add more features here based on your custom use case\n",
    "    features = [\n",
    "            'bias',\n",
    "            'word.lower=' + word.lower(),\n",
    "            'word[-3:]=' + word[-3:],\n",
    "            'word[-2:]=' + word[-2:],\n",
    "            'word.isupper=%s' % word.isupper(),\n",
    "            'word.istitle=%s' % word.istitle(),\n",
    "            '-1:word.isntitle=%s' % is_nametitle(word),\n",
    "            '-1:word.is_inheader=%s' % is_inheader(word,t),\n",
    "            '-1:word.is_infrom=%s' % is_infrom(word,t),\n",
    "            '-1:word.is_insign=%s' % is_insign(word,t)\n",
    "            #'word.isdigit=%s' % word.isdigit(),\n",
    "            #'postag=' + postag\n",
    "        ]\n",
    "    \n",
    "    # Features for words that are not at the beginning of a document\n",
    "    if i > 0:\n",
    "            word1 = doc[i-1][0]\n",
    "            postag1 = doc[i-1][1]\n",
    "            features.extend([\n",
    "                '-1:word.lower=' + word1.lower(),\n",
    "                '-1:word.istitle=%s' % word1.istitle(),\n",
    "                '-1:word.isupper=%s' % word1.isupper(),\n",
    "                '-1:word.isntitle=%s' % is_nametitle(word1),\n",
    "                '-1:word.isntitle=%s' % is_nametitle(word),\n",
    "                '-1:word.is_inheader=%s' % is_inheader(word,t),\n",
    "                '-1:word.is_infrom=%s' % is_infrom(word,t),\n",
    "                '-1:word.is_insign=%s' % is_insign(word,t),\n",
    "                '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '-1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "    \n",
    "    # Features for words that are not at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "            word1 = doc[i+1][0]\n",
    "            postag1 = doc[i+1][1]\n",
    "            features.extend([\n",
    "                '+1:word.lower=' + word1.lower(),\n",
    "                '+1:word.istitle=%s' % word1.istitle(),\n",
    "                '+1:word.isupper=%s' % word1.isupper(),\n",
    "                '-1:word.isntitle=%s' % is_nametitle(word1),\n",
    "                '-1:word.isntitle=%s' % is_nametitle(word),\n",
    "                '-1:word.is_inheader=%s' % is_inheader(word,t),\n",
    "                '-1:word.is_infrom=%s' % is_infrom(word,t),\n",
    "                '-1:word.is_insign=%s' % is_insign(word,t),\n",
    "                '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '+1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414ecdb-df4a-4f1e-a816-5abe319fd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the doc list which only tuple (word,tag) to also have part of speech tag eg. ('me', 'PRP', 'NA')\n",
    "data = []\n",
    "for i, doc in enumerate(docs):\n",
    "    tokens = [t for t, label in doc]    \n",
    "    tagged = nltk.pos_tag(tokens)    \n",
    "    data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49257b19-a5bc-4391-aaff-ae2f60dc5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the tdoc tuples which only had (word,tag) to also have part of speech tag eg. ('me', 'PRP', 'NA')\n",
    "tdata=[]    \n",
    "for i, doc in enumerate(tdocs):\n",
    "    tokens = [t for t, label in doc]    \n",
    "    tagged = nltk.pos_tag(tokens)    \n",
    "    tdata.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3e540-80be-4e92-9ddc-d9e404945ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#genrate features and extract labels for training data  \n",
    "X = [extract_features(doc,\"train\") for doc in data]\n",
    "y = [get_labels(doc) for doc in data]\n",
    "X_train, X_unused, y_train, y_unused = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "print (len (X_train))\n",
    "print (len (X_unused))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f2a45-6c47-429a-88ff-f4251bcce309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#genrate features and extract labels for training data  \n",
    "tX = [extract_features(doc,\"test\") for doc in tdata]\n",
    "ty = [get_labels(doc) for doc in tdata]\n",
    "#print (tX[0])\n",
    "X_test, X_unused, y_test, y_unused = train_test_split(tX, ty, test_size=0.01, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3ba5d-b1f5-4c3b-a9f1-ba080268c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a CRF Model\n",
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "# Submit training data to the trainer\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "# Set the parameters of the model\n",
    "trainer.set_params({\n",
    "    # coefficient for L1 penalty\n",
    "    'c1': 0.1,\n",
    "\n",
    "    # coefficient for L2 penalty\n",
    "    'c2': 0.01,  \n",
    "\n",
    "    # maximum number of iterations\n",
    "    'max_iterations': 70,\n",
    "\n",
    "    # whether to include transitions that\n",
    "    # are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "# Provide a file name as a parameter to the train function, such that\n",
    "# the model will be saved to the file when training is finished\n",
    "trainer.train('crf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67613c0c-ab3e-41a4-89f8-0b98233387bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test random sample \n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('crf.model')\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "#look at a random sample in the testing set\n",
    "#i = 0\n",
    "#for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "    #print(\"%s (%s)\" % (y, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b878c91-fe19-4617-a109-3a1a1e4302ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See how good the model is preforming \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a mapping of labels to indices\n",
    "labels = {\"NA\": 1, \"true_name\": 0}\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_test for tag in row])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=[\"true_name\", \"NA\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de116-6ceb-4911-bee7-a015840d9161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-machine] *",
   "language": "python",
   "name": "conda-env-.conda-machine-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
